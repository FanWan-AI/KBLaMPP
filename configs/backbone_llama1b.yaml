backbone: "LLM-Research/Llama-3.2-1B-Instruct"
d_model: 2048
# Top‑k gating dimension for keys/values; adjust to fit your GPU
d_k: 384
d_v: 384
# Sentence embedding model; must be reachable via HuggingFace
embedding_model: "BAAI/bge-small-en-v1.5"
d_ctx: 384
# In which layers to inject knowledge (0‑indexed)
inject_layers: [8]
# Number of nearest neighbours to fetch per token
K_top: 8
# Weights for context and time scores
gamma: 1.0
eta: 1.0
temperature: 1.0

# Training hyperparameters (stage A)
train:
  batch_size: 1
  max_seq_len: 512
  lr: 5e-4
  weight_decay: 0.01
  max_steps: 5000
  log_interval: 50
  save_interval: 1000
  grad_accum: 4
